{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzptQhdh0pUr"
      },
      "source": [
        "Deadline: Dec 11th 2024"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cj5YlgDmzYWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjBOCJZHeYpf"
      },
      "source": [
        "# Part I - Vaccine Development with Monte Carlo Policy Evaluation\n",
        "\n",
        "You are the CEO of a biotech company which is considering the development of a new vaccine. Starting at phase 0 (state 0), the drug develpment can stay in the same state or advance to \"phase 1  with promising results\" (state 1) or advance to \"phase 1 with disappointing results\" (state 2), or fail completely (state 4). At phase 1, the drug can stay in the same state, fail or become a success (state 3), in which case you will sell its patent to a big pharma company for \\$10 million.\n",
        "These state transitions happen from month to month, and at each state, you have the option to make an additional investment of \\$100,000, which increases the chances of success.\n",
        "\n",
        "After careful study, your analysts develop the program below to simulate different scenarios using statistical data from similar projects. For this question, you don't need to understand what is inside the program, only how to use it. If you want, you can can skip to the next cell.\n",
        "\n",
        "Use the Monte Carlo method to find the value functions induced by the following policies:\n",
        "\n",
        "\n",
        "a) Always invest\n",
        "\n",
        "b) Never invest\n",
        "\n",
        "Use a discount factor of 0.9960 to discount future rewards.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhyWvVsAeYAV"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnAvrShs6ecs"
      },
      "source": [
        "import numpy as np\n",
        "class MDP():\n",
        "  def __init__(self):\n",
        "    self.A = [0, 1]\n",
        "    self.S = [0, 1, 2, 3, 4]\n",
        "\n",
        "    P0 = np.array([[0.5, .15, .15, 0, .20],\n",
        "                   [0, .5, .0, .25, .25],\n",
        "                   [0, 0, .15, .05, .8],\n",
        "                   [0, 0, 0, 0, 1],\n",
        "                   [0, 0, 0, 0, 1]])\n",
        "\n",
        "    R0 = np.array([0, 0, 0, 10, 0])\n",
        "\n",
        "    P1 = np.array([[0.5, .25, .15, 0, .10],\n",
        "                   [0, .5, .0, .35, .15],\n",
        "                   [0, 0, .20, .05, .75],\n",
        "                   [0, 0, 0, 0, 1],\n",
        "                   [0, 0, 0, 0, 1]])\n",
        "\n",
        "    R1 = np.array([-0.1, -0.1, -0.1, 10, 0])\n",
        "\n",
        "    self.P = [P0, P1]\n",
        "    self.R = [R0, R1]\n",
        "\n",
        "  def step(self, s, a):\n",
        "    s_prime = np.random.choice(len(self.S), p=self.P[a][s])\n",
        "    R = self.R[a][s]\n",
        "    if s_prime == 4:\n",
        "      done = True\n",
        "    else:\n",
        "      done = False\n",
        "    return s_prime, R, done\n",
        "\n",
        "  def simulate(self, s, a, π):\n",
        "    done = False\n",
        "    t = 0\n",
        "    history = []\n",
        "    while not done:\n",
        "      if t > 0:\n",
        "        a = π[s]\n",
        "      s_prime, R, done = self.step(s, a)\n",
        "      history.append((s, a, R))\n",
        "      s = s_prime\n",
        "      t += 1\n",
        "\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-rfjh_37kmX"
      },
      "source": [
        "# Here is how you can use the program\n",
        "# First we create an instance of this MDP\n",
        "mdp = MDP()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRXv7TYB7h2_",
        "outputId": "57f420d5-7721-4532-fcb9-12ecbe0dfec5"
      },
      "source": [
        "# Now, starting at phase 0 (state s=0), we will choose to make additional investments (a=1).\n",
        "s = 0\n",
        "a = 1\n",
        "\n",
        "π = [0, 0, 0, 0]\n",
        "\n",
        "# We feed the state-action pair to the simulator and it tells us what happened in this simulation:\n",
        "history = mdp.simulate(s, a, π)\n",
        "\n",
        "history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 1, -0.1), (0, 0, 0), (2, 0, 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWfK-47V8I08"
      },
      "source": [
        "# In the simulation above, we have 3 observations of the triplet (state, action, reward)\n",
        "# Notice the reward of -0.1. This is because we chose to make additional investments (the units are in millions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}